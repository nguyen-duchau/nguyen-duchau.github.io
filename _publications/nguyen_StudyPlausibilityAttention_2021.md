---
title: "A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference"
collection: publications
permalink: /publication/nguyen_StudyPlausibilityAttention_2021
excerpt: #'This paper is about the number 3. The number 4 is left for future work.'
date: 13/12/2021
venue: '20th IEEE International Conference on Machine Learning and Applications (ICMLA)'
slidesurl: #'http://academicpages.github.io/files/slides3.pdf'
paperurl: 'https://hal.science/hal-03372669/document'
citation: 'Duc Hau Nguyen, Guillaume Gravier, Pascale Sébillot. A Study of the Plausibility of Attention between RNN Encoders in Natural Language Inference. In IEEE International Conference on Machine Learning and Applications (ICMLA), pp. 1623–1629, 2021.'
---

Abstract
======

Attention maps in neural models for NLP are appealing to explain the decision made by a model, hopefully emphasizing words that justify the decision. While many empirical studies hint that attention maps can provide such justification from the analysis of sound examples, only a few assess the plausibility of explanations based on attention maps, i.e., the usefulness of attention maps for humans to understand the decision. These studies furthermore focus on text classification. In this paper, we report on a preliminary assessment of attention maps in a sentence comparison task, namely natural language inference. We compare the cross-attention weights between two RNN encoders with human-based and heuristic-based annotations on the eSNLI corpus. We show that the heuristic reasonably correlates with human annotations and can thus facilitate evaluation of plausible explanations in sentence comparison tasks. Raw attention weights however remain only loosely related to a plausible explanation.