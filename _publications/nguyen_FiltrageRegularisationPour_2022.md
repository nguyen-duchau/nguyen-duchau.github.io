---
title: "Filtrage et régularisation pour améliorer la plausibilité des poids d'attention dans la tâche d'inférence en langue naturelle"
collection: publications
permalink: /publication/nguyen_FiltrageRegularisationPour_2022
excerpt: #'This paper is about fixing template issue #693.'
date: 17/10/2022
venue: '[JEP/TALN/RECITAL](https://aclanthology.org/venues/jeptalnrecital/)'
paperurl: 'https://aclanthology.org/2022.jeptalnrecital-taln.9.pdf'
citation: 'Duc Hau Nguyen, Guillaume Gravier, and Pascale Sébillot. Filtrage et régularisation pour améliorer la plausibilité des poids d’attention dans la tâche d’inférence en langue naturelle. Traitement Automatique des Langues Naturelles (TALN), pp. 95–103, 2022.'
---

Abstract (En)
======
We study the plausibility of the attention mechanism for a sentence inference task (entailment), i.e., its ability to provide a plausible explanation for a human regarding the relationship between two sentences. Based on the Explanable Stanford Natural Language Inference (e-SNLI) corpus, it has been shown that attention weights are generally not plausible in practice and tend not to focus on important tokens. In this paper, different approaches are suggested to make attention weights more plausible, relying on masks derived from morphosyntactic analysis or using regularization to enforce sparsity. We demonstrate that these strategies significantly improve the plausibility of attention weights and prove to be more effective than traditional saliency-map approaches.

Résumé (Fr)
======
Nous étudions la plausibilité d’un mécanisme d’attention pour une tâche d’inférence de phrases (entailment), c’est-à-dire sa capacité à fournir une explication plausible pour un humain de la relation entre deux phrases. En s’appuyant sur le corpus Explanation-Augmented Standford Natural Language Inference, il a été montré que les poids d’attention sont peu plausibles en pratique et tendent à ne pas se concentrer sur les tokens importants. Nous étudions ici différentes approches pour rendre les poids d’attention plus plausibles, en nous appuyant sur des masques issus d’une analyse morphosyntaxique ou sur une régularisation pour forcer la parcimonie. Nous montrons que ces stratégies permettent d’améliorer sensiblement la plausibilité des poids d’attention et s’avèrent plus performantes que les approches par carte de saillance.
